{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7860e9e0-b3c2-4e81-b3e7-c017d9eadcc5",
   "metadata": {},
   "source": [
    "# ğŸ‰ Welcome to Your First Assignment! ğŸ“šğŸ“\n",
    "\n",
    "Brace yourselfâ€”it's homework time! But donâ€™t worry, this is the fun kind. ğŸ˜„  \n",
    "Instructions are below. If you get stuck, feel free to peek into the solutions folder or ask me. (Yes, cheating is allowed... this time.) ğŸ˜‰  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ EXERCISE ASSIGNMENT\n",
    "\n",
    "### ğŸ“Œ Task: Upgrade Your First Project\n",
    "Make your webpage summarizer even cooler by running an **Open-Source Model locally via Ollama** instead of relying on OpenAIâ€™s APIs. No credit card bills, just pure, free AI goodness. ğŸ¤‘âœ¨\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ Why Bother?\n",
    "\n",
    "**Benefits:**  \n",
    "- ğŸ’¸ **No API charges:** Open-source = Free = Your wallet stays happy.  \n",
    "- ğŸ”’ **Data Privacy:** Everything stays on your computerâ€”no rogue robots stealing your secrets. ğŸ¤«  \n",
    "\n",
    "**Disadvantages:**  \n",
    "- ğŸ¦¥ **Less Power:** Open-Source Models are like adorable baby dragons compared to the Frontier Modelâ€™s fire-breathing beast. ğŸ‰ğŸ”¥  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”§ Recap on Installing Ollama\n",
    "\n",
    "1. **Step 1:** Visit [ollama.com](https://ollama.com) and install the Ollama software.  \n",
    "2. **Step 2:** Once complete, the Ollama server should already be running locally. To check, visit:  \n",
    "   ğŸ‘‰ [http://localhost:11434/](http://localhost:11434/)  \n",
    "\n",
    "   You should see the message:  \n",
    "   ```plaintext\n",
    "   Ollama is running.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c93d79-a15e-41df-891b-e3607bc85222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b926c8-b46a-45cf-b8c2-ecf1068c423c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39a629a-a5a7-43ae-8347-58a872164355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a messages list using the same format that we used for OpenAI\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Describe some of the business applications of Generative AI\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bec41f8-dfe0-4579-991e-d2e410d53074",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": False\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f27f58b-fc0b-41d3-8602-980fe709ea05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just make sure the model is loaded\n",
    "\n",
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1415a15b-3d5d-4837-865f-dcc90de056a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ˜… If this doesn't work for any reason\n",
    "# 1. Check your code, sometimes code just wants to keep you humble. ğŸ¤·â€â™‚ï¸\n",
    "# 2. Double-check the instructions under 'Recap on installation of Ollama' at the top of this lab. Seriously, it's like checking if your toaster is plugged in. ğŸ”ŒğŸ\n",
    "# 3. If NONE of that worksâ€”congratulations, youâ€™ve officially entered the â€œWhat The Heck?â€ Zone. ğŸ¢ \n",
    "#    Ask for Shekhar before you start considering sacrificing your keyboard to the AI gods. ğŸ™ğŸ’»ğŸ”¥\n",
    "\n",
    "response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)\n",
    "print(response.json().get('message', {}).get('content', \"ğŸ¤” Hmm... Something went wrong, and the AI refuses to explain itself.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d729fff-9b9d-4b20-926b-6bc2d226cc0e",
   "metadata": {},
   "source": [
    "## ğŸŒŸ Introducing the `ollama` Package\n",
    "\n",
    "Now, letâ€™s level up your AI wizardry by using the elegant `ollama` Python package instead of making clunky direct HTTP calls. ğŸ“¦âœ¨\n",
    "\n",
    "The magic? ğŸª„ Under the hood, itâ€™s still making the same call to the Ollama server running at `localhost:11434`.  \n",
    "But now, you get to do it with style and graceâ€”like riding a unicorn instead of a rusty tricycle. ğŸ¦„ğŸš²\n",
    "\n",
    "Ready to roll? Letâ€™s do this! ğŸš€\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66339501-e2c3-4c9a-930e-c4658dcbf97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(model=MODEL, messages=messages)\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34a39cd-d6f7-41aa-8025-a04eb08c693b",
   "metadata": {},
   "source": [
    "## ğŸ”„ Alternative Approach - Using the OpenAI Python Library to Connect to Ollama\n",
    "\n",
    "Hereâ€™s another trick up your sleeve: Instead of using raw HTTP calls, you can connect to Ollama using the **OpenAI Python library**. ğŸâœ¨\n",
    "\n",
    "Why? Because sometimes itâ€™s nice to let a library handle the boring stuffâ€”like setting headers and dealing with weird error messagesâ€”while you bask in the glory of smooth, streamlined code. ğŸ˜ğŸ“ˆ\n",
    "\n",
    "And donâ€™t worry, itâ€™s still hitting the same Ollama server at `localhost:11434`. Just with a bit more finesse. ğŸ’…\n",
    "\n",
    "Letâ€™s make some AI magic happen! ğŸš€\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d40e9e3-35f3-460f-9ae8-4dd390690696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's actually an alternative approach that some people might prefer\n",
    "# You can use the OpenAI client python library to call Ollama:\n",
    "\n",
    "from openai import OpenAI\n",
    "ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "\n",
    "response = ollama_via_openai.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1d128a-8dfc-414c-b2c1-3c49010f99e8",
   "metadata": {},
   "source": [
    "## ğŸ¤” Wait... Why Does That Work? \n",
    "\n",
    "Yeah, it sounds a bit bonkers, right? ğŸ¤ª You just used **OpenAI code** to call **Ollama**. Like trying to order pizza with an Uber app. So, whatâ€™s the deal here? ğŸ•ğŸš•ğŸ¤–\n",
    "\n",
    "### ğŸ“– Hereâ€™s the Deal:\n",
    "\n",
    "The `OpenAI` Python class is **just a fancy piece of code** that OpenAIâ€™s engineers wrote to make API calls.  \n",
    "Itâ€™s like a super-convenient mailman that delivers your requests to the right place. ğŸ“¬âœ¨\n",
    "\n",
    "### ğŸ” Whatâ€™s Actually Happening Under the Hood:\n",
    "\n",
    "- When you call:\n",
    "  ```python\n",
    "  openai.ChatCompletion.create()\n",
    "- This code simply makes a web request to this URL:\n",
    "  ```plaintext\n",
    "  https://api.openai.com/v1/chat/completions\n",
    "- This â€œclient libraryâ€ is just wrapper code that handles all the boring details of making web requests and parsing responses.\n",
    "- The real heavy-lifting happens on OpenAIâ€™s cloud servers, not your local machine. Your code just sends a polite â€œHey, can you do this for me?â€ request. ğŸ™‹â€â™‚ï¸ğŸ“¡ğŸŒ©ï¸\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f418f1c8-ee2c-4ace-91b3-56336aa225c1",
   "metadata": {},
   "source": [
    "## ğŸ§© Why Does This Work With Ollama?\n",
    "\n",
    "OpenAIâ€™s API approach became so popular that **other AI providers decided to copy their playbook**.  \n",
    "Instead of reinventing the wheel, they built **compatible endpoints** that look and act just like OpenAIâ€™s. ğŸš€\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”Œ Ollamaâ€™s Trick\n",
    "\n",
    "Ollama hosts a server **locally** on your machine at:  \n",
    "```plaintext\n",
    "http://localhost:11434/v1/chat/completions\n",
    "\n",
    "And guess what? Ollama speaks the same API language as OpenAIâ€™s API! ğŸ¤\n",
    "Itâ€™s like teaching your cat to bark just so the dog will listen. ğŸ±â¡ï¸ğŸ¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61c26d6-1c51-4a2f-97e7-503cd072759c",
   "metadata": {},
   "source": [
    "### ğŸ’¡ The Clever Twist\n",
    "\n",
    "The engineers at OpenAI had a *brilliant* idea:  \n",
    "\n",
    "> **â€œWhy not let developers change the `base_url` and call compatible APIs with our client library?â€** ğŸ©âœ¨  \n",
    "\n",
    "So, they made it possible for you to do this:  \n",
    "```python\n",
    "ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "\n",
    "This command reroutes your requests to Ollamaâ€™s endpoint instead of OpenAIâ€™sâ€”like swapping the address on a letter before mailing it. ğŸ“¬ğŸ“¦\n",
    "\n",
    "And thatâ€™s it! No dark magic, no wizard contract with the AI overlordsâ€”just a clever little trick that makes your life easier. ğŸ˜„ğŸ”‘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be3f28d-7bc0-4961-b416-f3a205d7dc85",
   "metadata": {},
   "source": [
    "### ğŸš€ So Why Does This Matter?\n",
    "\n",
    "Now you can use the same **OpenAI client library** to talk to **Ollama, Gemini, DeepSeek**, or pretty much any other compatible AI model out there.  \n",
    "It's like having a **universal remote for AI**. ğŸ“ºğŸ¤–âœ¨  \n",
    "\n",
    "Next up: **Letâ€™s take this superpower and build something awesome!** ğŸ’ªğŸ”¥  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1794dff7-164b-4b3e-8e12-b9621650f628",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
