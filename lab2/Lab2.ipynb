{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7860e9e0-b3c2-4e81-b3e7-c017d9eadcc5",
   "metadata": {},
   "source": [
    "# 🎉 Welcome to Your First Assignment! 📚📝\n",
    "\n",
    "Brace yourself—it's homework time! But don’t worry, this is the fun kind. 😄  \n",
    "Instructions are below. If you get stuck, feel free to peek into the solutions folder or ask me. (Yes, cheating is allowed... this time.) 😉  \n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 EXERCISE ASSIGNMENT\n",
    "\n",
    "### 📌 Task: Upgrade Your First Project\n",
    "Make your webpage summarizer even cooler by running an **Open-Source Model locally via Ollama** instead of relying on OpenAI’s APIs. No credit card bills, just pure, free AI goodness. 🤑✨\n",
    "\n",
    "---\n",
    "\n",
    "### 🎁 Why Bother?\n",
    "\n",
    "**Benefits:**  \n",
    "- 💸 **No API charges:** Open-source = Free = Your wallet stays happy.  \n",
    "- 🔒 **Data Privacy:** Everything stays on your computer—no rogue robots stealing your secrets. 🤫  \n",
    "\n",
    "**Disadvantages:**  \n",
    "- 🦥 **Less Power:** Open-Source Models are like adorable baby dragons compared to the Frontier Model’s fire-breathing beast. 🐉🔥  \n",
    "\n",
    "---\n",
    "\n",
    "### 🔧 Recap on Installing Ollama\n",
    "\n",
    "1. **Step 1:** Visit [ollama.com](https://ollama.com) and install the Ollama software.  \n",
    "2. **Step 2:** Once complete, the Ollama server should already be running locally. To check, visit:  \n",
    "   👉 [http://localhost:11434/](http://localhost:11434/)  \n",
    "\n",
    "   You should see the message:  \n",
    "   ```plaintext\n",
    "   Ollama is running.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c93d79-a15e-41df-891b-e3607bc85222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b926c8-b46a-45cf-b8c2-ecf1068c423c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39a629a-a5a7-43ae-8347-58a872164355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a messages list using the same format that we used for OpenAI\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Describe some of the business applications of Generative AI\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bec41f8-dfe0-4579-991e-d2e410d53074",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": False\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f27f58b-fc0b-41d3-8602-980fe709ea05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just make sure the model is loaded\n",
    "\n",
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1415a15b-3d5d-4837-865f-dcc90de056a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 😅 If this doesn't work for any reason\n",
    "# 1. Check your code, sometimes code just wants to keep you humble. 🤷‍♂️\n",
    "# 2. Double-check the instructions under 'Recap on installation of Ollama' at the top of this lab. Seriously, it's like checking if your toaster is plugged in. 🔌🍞\n",
    "# 3. If NONE of that works—congratulations, you’ve officially entered the “What The Heck?” Zone. 🎢 \n",
    "#    Ask for Shekhar before you start considering sacrificing your keyboard to the AI gods. 🙏💻🔥\n",
    "\n",
    "response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)\n",
    "print(response.json().get('message', {}).get('content', \"🤔 Hmm... Something went wrong, and the AI refuses to explain itself.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d729fff-9b9d-4b20-926b-6bc2d226cc0e",
   "metadata": {},
   "source": [
    "## 🌟 Introducing the `ollama` Package\n",
    "\n",
    "Now, let’s level up your AI wizardry by using the elegant `ollama` Python package instead of making clunky direct HTTP calls. 📦✨\n",
    "\n",
    "The magic? 🪄 Under the hood, it’s still making the same call to the Ollama server running at `localhost:11434`.  \n",
    "But now, you get to do it with style and grace—like riding a unicorn instead of a rusty tricycle. 🦄🚲\n",
    "\n",
    "Ready to roll? Let’s do this! 🚀\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66339501-e2c3-4c9a-930e-c4658dcbf97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(model=MODEL, messages=messages)\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34a39cd-d6f7-41aa-8025-a04eb08c693b",
   "metadata": {},
   "source": [
    "## 🔄 Alternative Approach - Using the OpenAI Python Library to Connect to Ollama\n",
    "\n",
    "Here’s another trick up your sleeve: Instead of using raw HTTP calls, you can connect to Ollama using the **OpenAI Python library**. 🐍✨\n",
    "\n",
    "Why? Because sometimes it’s nice to let a library handle the boring stuff—like setting headers and dealing with weird error messages—while you bask in the glory of smooth, streamlined code. 😎📈\n",
    "\n",
    "And don’t worry, it’s still hitting the same Ollama server at `localhost:11434`. Just with a bit more finesse. 💅\n",
    "\n",
    "Let’s make some AI magic happen! 🚀\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d40e9e3-35f3-460f-9ae8-4dd390690696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's actually an alternative approach that some people might prefer\n",
    "# You can use the OpenAI client python library to call Ollama:\n",
    "\n",
    "from openai import OpenAI\n",
    "ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "\n",
    "response = ollama_via_openai.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1d128a-8dfc-414c-b2c1-3c49010f99e8",
   "metadata": {},
   "source": [
    "## 🤔 Wait... Why Does That Work? \n",
    "\n",
    "Yeah, it sounds a bit bonkers, right? 🤪 You just used **OpenAI code** to call **Ollama**. Like trying to order pizza with an Uber app. So, what’s the deal here? 🍕🚕🤖\n",
    "\n",
    "### 📖 Here’s the Deal:\n",
    "\n",
    "The `OpenAI` Python class is **just a fancy piece of code** that OpenAI’s engineers wrote to make API calls.  \n",
    "It’s like a super-convenient mailman that delivers your requests to the right place. 📬✨\n",
    "\n",
    "### 🔍 What’s Actually Happening Under the Hood:\n",
    "\n",
    "- When you call:\n",
    "  ```python\n",
    "  openai.ChatCompletion.create()\n",
    "- This code simply makes a web request to this URL:\n",
    "  ```plaintext\n",
    "  https://api.openai.com/v1/chat/completions\n",
    "- This “client library” is just wrapper code that handles all the boring details of making web requests and parsing responses.\n",
    "- The real heavy-lifting happens on OpenAI’s cloud servers, not your local machine. Your code just sends a polite “Hey, can you do this for me?” request. 🙋‍♂️📡🌩️\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f418f1c8-ee2c-4ace-91b3-56336aa225c1",
   "metadata": {},
   "source": [
    "## 🧩 Why Does This Work With Ollama?\n",
    "\n",
    "OpenAI’s API approach became so popular that **other AI providers decided to copy their playbook**.  \n",
    "Instead of reinventing the wheel, they built **compatible endpoints** that look and act just like OpenAI’s. 🚀\n",
    "\n",
    "---\n",
    "\n",
    "### 🔌 Ollama’s Trick\n",
    "\n",
    "Ollama hosts a server **locally** on your machine at:  \n",
    "```plaintext\n",
    "http://localhost:11434/v1/chat/completions\n",
    "\n",
    "And guess what? Ollama speaks the same API language as OpenAI’s API! 🎤\n",
    "It’s like teaching your cat to bark just so the dog will listen. 🐱➡️🐶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61c26d6-1c51-4a2f-97e7-503cd072759c",
   "metadata": {},
   "source": [
    "### 💡 The Clever Twist\n",
    "\n",
    "The engineers at OpenAI had a *brilliant* idea:  \n",
    "\n",
    "> **“Why not let developers change the `base_url` and call compatible APIs with our client library?”** 🎩✨  \n",
    "\n",
    "So, they made it possible for you to do this:  \n",
    "```python\n",
    "ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "\n",
    "This command reroutes your requests to Ollama’s endpoint instead of OpenAI’s—like swapping the address on a letter before mailing it. 📬📦\n",
    "\n",
    "And that’s it! No dark magic, no wizard contract with the AI overlords—just a clever little trick that makes your life easier. 😄🔑"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be3f28d-7bc0-4961-b416-f3a205d7dc85",
   "metadata": {},
   "source": [
    "### 🚀 So Why Does This Matter?\n",
    "\n",
    "Now you can use the same **OpenAI client library** to talk to **Ollama, Gemini, DeepSeek**, or pretty much any other compatible AI model out there.  \n",
    "It's like having a **universal remote for AI**. 📺🤖✨  \n",
    "\n",
    "Next up: **Let’s take this superpower and build something awesome!** 💪🔥  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1794dff7-164b-4b3e-8e12-b9621650f628",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
