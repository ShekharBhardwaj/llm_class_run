{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "144a754c-478d-4863-93ee-0470a6c79962",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# ðŸ§  Playing with Multiple Models and Styles\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Mission\n",
    "\n",
    "In this lab, you will:\n",
    "\n",
    "- Send a company name + website to different AI models\n",
    "- Watch how **different models** (GPT, Claude, Gemini, Ollama) respond\n",
    "- Control **how they behave** (funny, sincere, argumentative, or crazy)\n",
    "- And see how **one simple input** can create **very different outputs** depending on the model and style!\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ› ï¸ Whatâ€™s Going On\n",
    "\n",
    "| Step | What Weâ€™re Doing |\n",
    "|:---|:---|\n",
    "| 1 | Pick an AI model (GPT, Claude, Gemini, Ollama) |\n",
    "| 2 | Pick a style (funny, sincere, crazy, argumentative) |\n",
    "| 3 | AI reads the basic webpage info |\n",
    "| 4 | AI writes a short company brochure â€” with your chosen style |\n",
    "| 5 | Everything shows up nicely in a Gradio web app |\n",
    "\n",
    "âœ… Itâ€™s not about scraping â€” it's about **how AIs think and speak differently**!\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¥ Key Parts of the Code\n",
    "\n",
    "| Part | What It Does |\n",
    "|:---|:---|\n",
    "| `Website` class | Grabs basic text (so AI has something to talk about) |\n",
    "| `get_system_prompt(style)` | Tells the AI how to \"act\" |\n",
    "| `invoke_*` functions | Talk to different models (GPT, Claude, Gemini, Ollama) |\n",
    "| `gradio_app` | Launches a simple app you can actually click and play with |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Why This Matters\n",
    "\n",
    "- **Multi-model thinking**: Not all AIs behave the same.  \n",
    "- **Prompt engineering**: How you *ask* changes what you *get*.  \n",
    "- **Controlled creativity**: You can guide AI tone and style â€” it's not just random.\n",
    "\n",
    "âœ… These are **foundational skills** for anything youâ€™ll build in AI later â€” apps, agents, assistants, and beyond.\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸŽ¯ Final Thought\n",
    "\n",
    "> **Today: Pick a model, pick a style.  \n",
    "> Tomorrow: Build AI that fits your brand, your users, and your dreams.**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a951df1f-0b29-477e-bb8f-aad8e95f0687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up all the imports we need\n",
    "# (a.k.a. assembling the Avengers for our app)\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import google.generativeai\n",
    "import anthropic\n",
    "import gradio as gr\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42642d7-b75a-4b07-9474-365bd4486c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load secret keys from the .env file\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6852d887-7034-4ddc-a0c6-b34f66b918d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up clients for OpenAI, Claude, and Gemini\n",
    "openai = OpenAI()\n",
    "claude = anthropic.Anthropic()\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc612a5-7018-4fac-8c19-a9cc60bbc8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama is running locally (like your friendly neighborhood model)\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a31df0-16ed-46d7-9a0a-f5b7002a51f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ¡ Scraping a website class\n",
    "class Website:\n",
    "    url: str\n",
    "    title: str\n",
    "    text: str\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        response = requests.get(url)\n",
    "        self.body = response.content\n",
    "        soup = BeautifulSoup(self.body, 'html.parser')\n",
    "\n",
    "        # Grab the title (or improvise if missing)\n",
    "        self.title = soup.title.string if soup.title else \"No title found\"\n",
    "\n",
    "        # Clean out junk we don't want: scripts, images, inputs\n",
    "        for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "            irrelevant.decompose()\n",
    "\n",
    "        # Get readable page text\n",
    "        self.text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "    def get_contents(self):\n",
    "        return f\"Webpage Title:\\n{self.title}\\nWebpage Contents:\\n{self.text}\\n\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606588d4-e187-421d-a38f-b1a0a00f0d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ¨ Craft a system prompt based on style\n",
    "def get_system_prompt(style: str) -> str:\n",
    "    return f\"\"\"You are a {style} assistant that analyzes a company's landing page \\\n",
    "and creates a short brochure for customers, investors, and recruits. Respond in markdown.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f137a6-6c70-41e2-b9b6-bbaa92a5ed57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”¥ OpenAI GPT - stream responses\n",
    "def invoke_GPT(style: str, user_prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": get_system_prompt(style)},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    stream = openai.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "    result = \"\"\n",
    "    for chunk in stream:\n",
    "        result += chunk.choices[0].delta.content or \"\"\n",
    "        yield result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34e9683-41bc-49c8-865d-41b3c3fafebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§Š Claude model - stream responses\n",
    "def invoke_claude(style: str, user_prompt):\n",
    "    result = claude.messages.stream(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        max_tokens=1000,\n",
    "        temperature=0.7,\n",
    "        system=get_system_prompt(style),\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "    )\n",
    "    response = \"\"\n",
    "    with result as stream:\n",
    "        for text in stream.text_stream:\n",
    "            response += text or \"\"\n",
    "            yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbe2acc-3ca0-4e70-b105-01acf11681d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŒŸ Gemini model - a slightly fancier setup\n",
    "def invoke_gemini(style: str, user_prompt: str):\n",
    "    gemini_via_openai_client = OpenAI(\n",
    "        api_key=google_api_key,\n",
    "        base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": get_system_prompt(style)},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    response = gemini_via_openai_client.chat.completions.create(\n",
    "        model=\"gemini-1.5-flash\",\n",
    "        messages=messages\n",
    "    )\n",
    "    result = response.choices[0].message.content\n",
    "    yield result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8191d690-6d63-4d9c-82b7-a7e9fccbff27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ¦™ Ollama (local llama model) - stream responses\n",
    "def invoke_ollama(style: str, user_prompt: str):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": get_system_prompt(style)},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    response = requests.post(\n",
    "        OLLAMA_API,\n",
    "        headers=HEADERS,\n",
    "        json={\n",
    "            \"model\": MODEL,\n",
    "            \"messages\": messages,\n",
    "            \"stream\": True\n",
    "        },\n",
    "        stream=True\n",
    "    )\n",
    "    result = \"\"\n",
    "    for chunk in response.iter_lines():\n",
    "        if chunk:\n",
    "            chunk_data = json.loads(chunk.decode('utf-8'))\n",
    "            if 'message' in chunk_data and 'content' in chunk_data['message']:\n",
    "                result += chunk_data['message']['content']\n",
    "                yield result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08de9d89-20bf-4a5e-b853-ed3167327059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ² Model selector based on user choice\n",
    "def model_case(model: str, style: str, user_prompt: str):\n",
    "    model = model.lower()\n",
    "    match model:\n",
    "        case \"gpt\":\n",
    "            return invoke_GPT(style, user_prompt)\n",
    "        case \"claude\":\n",
    "            return invoke_claude(style, user_prompt)\n",
    "        case \"gemini\":\n",
    "            return invoke_gemini(style, user_prompt)\n",
    "        case \"ollama\":\n",
    "            return invoke_ollama(style, user_prompt)\n",
    "        case \"_\":\n",
    "            pass  # silently fail if unknown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d422184-4264-49cf-b2ee-daa614d72bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ—ï¸ Building the final company brochure\n",
    "def stream_brochure(company_name, url, model, style):\n",
    "    prompt = f\"Please generate a company brochure for {company_name}. Here is their landing page:\\n\"\n",
    "    prompt += Website(url).get_contents()\n",
    "    yield from model_case(model, style, prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d17644-3f4b-41c6-af04-175b1aa9d00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ›ï¸ Gradio app to tie everything together\n",
    "def gradio_app():\n",
    "    view = gr.Interface(\n",
    "        fn=stream_brochure,\n",
    "        inputs=[\n",
    "            gr.Textbox(label=\"Company name:\"),\n",
    "            gr.Textbox(label=\"Landing page URL including http:// or https://\"),\n",
    "            gr.Dropdown([\"GPT\", \"Claude\", \"Gemini\", \"Ollama\"], label=\"Select model\"),\n",
    "            gr.Dropdown([\"funny\", \"sincere\", \"argumentative\", \"crazy\"], label=\"Select style\")\n",
    "        ],\n",
    "        outputs=[gr.Markdown(label=\"Brochure:\")],\n",
    "        flagging_mode=\"never\"\n",
    "    )\n",
    "    return view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f7ea5e-c46e-4e45-a6ab-00c9c79ccdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ðŸ off to the races!\n",
    "gradio_app()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9435866c-ef12-4a1d-8ae0-24a91d931bd2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ› ï¸ Mini Challenge: Add a New Style to Your Gradio App\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Your Mission\n",
    "\n",
    "Your Gradio app is already great â€”  \n",
    "Now let's **level it up** even more!\n",
    "\n",
    "âœ… **Add one more style option** for the AI to use.\n",
    "\n",
    "âœ… **Add a new dropdown** choice in Gradio so users can pick this new setting.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¥ What You Need to Do\n",
    "\n",
    "1. **Pick a new style** for your assistant.  \n",
    "   *(Example ideas: professional, sarcastic, poetic, motivational... up to you!)*\n",
    "\n",
    "2. **Update the dropdown** list in the `gr.Interface` inside `gradio_app()`.\n",
    "\n",
    "3. **No code changes needed** elsewhere!  \n",
    "   (The system prompt builder will automatically handle your new style.)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Why This Matters\n",
    "\n",
    "- This teaches you how to **extend your AI's personality**.  \n",
    "- It shows how **small changes to prompts** create **big changes in outputs**.\n",
    "\n",
    "âœ… Building flexible, user-driven AIs is how real-world assistants work!\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸš€ Final Hint\n",
    "\n",
    "> **If you can control the style, you can control the entire vibe of your AI.**\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
