{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fb599d8-5b83-4222-a0c3-4e0f64130e09",
   "metadata": {},
   "source": [
    "<img src=\"../ContextWindow.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6620bfdf-1ad9-4fae-ba5f-3f05ee576646",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ§  Understanding the Context Window in Language Models\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ What is the Context Window?\n",
    "\n",
    "- The **context window** is the **maximum number of tokens** an LLM (Large Language Model) can **look at** when **predicting the next token**.\n",
    "- It includes:\n",
    "  - The **input prompt** (what you send)\n",
    "  - **System prompts** (background instructions)\n",
    "  - **Previous user and assistant responses** (in a conversation)\n",
    "\n",
    "âœ… **In short:**  \n",
    "> It's the **total memory** the model can \"see\" at once before making its next move.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Why is the Context Window Important?\n",
    "\n",
    "- LLMs **predict the next token** based on everything they've seen **within** the context window.\n",
    "- If information falls **outside** the context window, it is **forgotten**.\n",
    "- A **bigger context window** means:\n",
    "  - Longer conversations.\n",
    "  - Handling larger documents.\n",
    "  - Better memory across interactions.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Real Chat Example: What Happens Behind the Scenes\n",
    "\n",
    "When you use ChatGPT:\n",
    "\n",
    "| Step | What Happens |\n",
    "|:---|:---|\n",
    "| You send a question | Your message is tokenized. |\n",
    "| ChatGPT answers | Its response is also tokenized and remembered. |\n",
    "| You ask another question | The entire past conversation is re-sent to the model! |\n",
    "| ChatGPT responds again | It predicts the next token based on **all prior tokens**. |\n",
    "\n",
    "âœ… **Important:**  \n",
    "> ChatGPT doesn't truly \"remember\" â€” it **reprocesses the whole conversation** every time!\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Example: Growing Context During a Chat\n",
    "\n",
    "Imagine this flow:\n",
    "\n",
    "1. **System prompt**: \"You are a helpful assistant.\"\n",
    "2. **User prompt**: \"Summarize this website.\"\n",
    "3. **AI Response**: \"Here's the summary...\"\n",
    "4. **User prompt**: \"Now explain it in simple terms.\"\n",
    "5. **AI Response**: \"Here's a simple explanation...\"\n",
    "\n",
    "ðŸ”” Every message adds **more tokens** into the conversation history!\n",
    "\n",
    "âœ… **Result:**  \n",
    "- Over time, the context window **fills up**.\n",
    "- If the conversation is **too long**, the oldest parts may get **truncated** or **forgotten**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Real-World Size Example\n",
    "\n",
    "| Example | Approximate Token Count |\n",
    "|:---|:---|\n",
    "| A short email | ~200 tokens |\n",
    "| A chapter of a book | ~5,000â€“10,000 tokens |\n",
    "| Complete Works of Shakespeare | ~1.2 million tokens |\n",
    "\n",
    "âœ… **Insight:**  \n",
    "- To ask a question about the **entire works of Shakespeare**, the model must be able to hold **1.2 million tokens** at once!\n",
    "- **Most current LLMs can't handle that yet** â€” unless specially designed (like Gemini 1.5 Pro with 1M tokens!).\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Summary: Context Window in a Nutshell\n",
    "\n",
    "> **The context window is how much conversation the AI can see at once to predict the next thing.**  \n",
    "> **Bigger window = longer memory. Smaller window = short-term memory.**\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸŽ¯ Quick Takeaways\n",
    "\n",
    "| | |\n",
    "|:---|:---|\n",
    "| Context window includes all past prompts + responses | |\n",
    "| ChatGPT \"remembers\" by reprocessing conversation history | |\n",
    "| If conversation gets too long, old parts get dropped | |\n",
    "| More context = better multi-turn chats, document reasoning | |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3c6225-ede6-4e4c-9b17-b22a49599d35",
   "metadata": {},
   "source": [
    "Let's look at the **Leaderboard:** https://www.vellum.ai/llm-leaderboard\n",
    "- Model Comparison\n",
    "- Context window, cost and speed comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9046bde-42eb-466d-ad6f-9e24d5992c06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
