{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5752d0c-e86d-4b31-a1ee-4cf41863ff72",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ§  Understanding Tokens in Language Models\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ What Are Tokens?\n",
    "\n",
    "- **Tokens** are **the smallest units** that are passed into a model.\n",
    "- A token could be a **character**, a **whole word**, or **part of a word**.\n",
    "\n",
    "âœ… **Example:**  \n",
    "- \"cat\" could be one token.  \n",
    "- \"handcrafted\" could be broken into \"hand\" + \"crafted\" as two tokens.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Early Tokenization Methods\n",
    "\n",
    "### 1. Character-Level Tokenization\n",
    "- **Old models** trained **character by character**.\n",
    "- Input = individual letters (a, b, c, ...).\n",
    "- **Benefits:**\n",
    "  - Very small vocabulary (just letters and symbols).\n",
    "- **Challenges:**\n",
    "  - Model must **learn how letters form words**, which was too complex.\n",
    "\n",
    "âœ… **Example:**  \n",
    "\"c\", \"a\", \"t\" â†’ the model had to learn this becomes \"cat\".\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Word-Level Tokenization\n",
    "- Models later trained **word by word**.\n",
    "- A vocabulary (**vocab**) was built with **every possible word**.\n",
    "- **Benefits:**\n",
    "  - Model knows the **meaning** of whole words easily.\n",
    "- **Challenges:**\n",
    "  - Huge vocabulary size.\n",
    "  - Problems with rare or new words (names, made-up words).\n",
    "\n",
    "âœ… **Example:**  \n",
    "The word \"cat\" would be one token, but a rare name like \"Musterers\" might not exist in vocab.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ The Breakthrough: Subword Tokenization\n",
    "\n",
    "- Instead of only characters or full words, **chunks of words** became tokens.\n",
    "- Chunks could represent:\n",
    "  - **Full words**\n",
    "  - **Part of words**\n",
    "  - **Word stems**\n",
    "\n",
    "âœ… **Example:**  \n",
    "- \"handcrafted\" â†’ \"hand\" + \"crafted\"  \n",
    "- \"musterers\" â†’ \"master\" + \"ers\"\n",
    "\n",
    "**Benefits:**\n",
    "- Handles rare and invented words better.\n",
    "- Smaller vocab size compared to word-level.\n",
    "- Captures meaning more effectively.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Real Examples Using GPT Tokenizer\n",
    "\n",
    "### Simple Sentence\n",
    "- Input: \"An important sentence for AI engineers.\"\n",
    "- Result:  \n",
    "  Each common word (\"an\", \"important\", \"sentence\", etc.) **mapped exactly to one token**.\n",
    "\n",
    "âœ… **Note:**  \n",
    "Even spaces (gaps between words) are part of tokens!\n",
    "\n",
    "---\n",
    "\n",
    "### Rare and Made-up Words\n",
    "\n",
    "| Word | How it Breaks into Tokens |\n",
    "|:---|:---|\n",
    "| exquisitely | `ex`, `quisite`, `ly` |\n",
    "| handcrafted | `hand`, `crafted` |\n",
    "| musterers | `master`, `ers` |\n",
    "| witchcraft | `witch`, `craft` |\n",
    "\n",
    "âœ… **Insights:**\n",
    "- **\"exquisitely\"** is not stored as a single token.  \n",
    "  It is broken into parts like: **\"ex\"**, **\"quisite\"**, **\"ly\"**.\n",
    "- **\"handcrafted\"** is broken into: **\"hand\"** and **\"crafted\"**.\n",
    "- **\"musterers\"** splits into: **\"master\"** and **\"ers\"** â€” preserving the verb **\"master\"** inside.\n",
    "- **\"witchcraft\"** splits into: **\"witch\"** and **\"craft\"** â€” preserving semantic meaning.\n",
    "\n",
    "---\n",
    "\n",
    "### Numbers Example\n",
    "- Input: \"6534589793238462643383...\"\n",
    "- Result:\n",
    "  - Long numbers are **broken into multiple tokens**.\n",
    "  - In GPT-2 tokenizer, about **three-digit groups** map to one token.\n",
    "\n",
    "âœ… **Note:**  \n",
    "Handling numbers increases token counts quickly!\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Quick Rules of Thumb\n",
    "\n",
    "| | |\n",
    "|:---|:---|\n",
    "| 1 token â‰ˆ 4 characters | (average English text) |\n",
    "| 1 token â‰ˆ 0.75 words | |\n",
    "| 1000 tokens â‰ˆ 750 words | |\n",
    "\n",
    "âœ… **Example:**  \n",
    "- The complete works of Shakespeare = about **1.2 million tokens** (~900,000 words).\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ How Many Tokens Can Popular Models Handle?\n",
    "\n",
    "| Model | Max Token Limit | Notes |\n",
    "|:---|:---|:---|\n",
    "| GPT-3.5-turbo | 4,096 tokens | About 3,000 words |\n",
    "| GPT-4 (8K context) | 8,192 tokens | About 6,000 words |\n",
    "| GPT-4 (32K context) | 32,768 tokens | About 24,000 words |\n",
    "| Claude 2 | 100,000 tokens | Can take almost an entire book in one prompt! |\n",
    "| Gemini 1.5 Pro | 1 million tokens | Experimental â€” whole large documents at once! |\n",
    "| LLaMA 2-13B | 4,096 tokens | Smaller context window compared to frontier models |\n",
    "| Mistral 7B | 8,192 tokens | Good balance of speed and size |\n",
    "\n",
    "âœ… **Important:**  \n",
    "- **More tokens = model can \"remember\" bigger prompts**.\n",
    "- **Bigger context windows = better long conversations, summarization, reasoning.**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Tokenizers Vary Across Models\n",
    "\n",
    "- **Different models use different tokenization rules**.\n",
    "- Early models: 1 letter = 1 token.\n",
    "- Modern models (like GPT, LLaMA) use **subword tokenization**.\n",
    "\n",
    "âœ… **Note:**  \n",
    "More tokens â‰  better.  \n",
    "Fewer tokens â‰  better.  \n",
    "It depends on model design, parameter size, and training goals.\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸŽ¯ Final Thought\n",
    "\n",
    "> **Tokenization is the art of chopping text into smart pieces so that AI can understand and predict better!**  \n",
    "> It's the bridge between human words and machine learning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1c8928-a4ea-48d4-9fda-8bc163488ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
