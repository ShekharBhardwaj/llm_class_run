{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de5eb5aa-02f3-4400-8c69-bbd6324855cc",
   "metadata": {},
   "source": [
    "<img src=\"../ai_is_a_joke.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc89b50-876e-47dd-92b1-ab8d165ae000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44365c2b-f6a1-4b14-828f-04743664a113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ Importing Google Gemini Magic\n",
    "# Sometimes this line throws a tantrum or crashes the kernel. üò§üí•\n",
    "# If it happens, don't worry ‚Äî just skip it!\n",
    "# ü™Ñ I'll show you an alternative Gemini spell later.\n",
    "\n",
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a348691-a488-4254-9838-d0518ff78918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ü™Ñ Loading secret keys from the magical .env scroll\n",
    "# üîç Peeking at the first few characters (no full secrets, promise!)\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"üß† OpenAI key found! Starts with: {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è OpenAI key missing!\")\n",
    "\n",
    "if anthropic_api_key:\n",
    "    print(f\"ü§ñ Anthropic key found! Starts with: {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Anthropic key missing!\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"üîÆ Google key found! Starts with: {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Google key missing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aba3dc2-aeb4-4181-a426-5ad9a9a74e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b9aa58-9342-4084-a4aa-f5749cb71a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üßô Setting up Google Gemini magic\n",
    "# If this causes trouble, no worries ‚Äî we'll have a secret backdoor later! üö™‚ú®\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592dafd7-c6e0-40e6-a1fe-43ff43f2aa57",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéØ Asking LLMs to Tell a Joke (and Beyond)\n",
    "\n",
    "While you *can* ask LLMs to tell a joke,  \n",
    "**they usually shine in much bigger ways!** üåü  \n",
    "Instead of just jokes, we'll soon explore how to:\n",
    "\n",
    "- Draft creative stories\n",
    "- Summarize huge articles\n",
    "- Answer technical questions\n",
    "- Help brainstorm new ideas!\n",
    "\n",
    "---\n",
    "\n",
    "# üß© What Information Is Passed to the API?\n",
    "\n",
    "When calling an LLM through an API, you typically send:\n",
    "\n",
    "| Item | Purpose |\n",
    "|:---|:---|\n",
    "| Model Name | Which version of the model to use (e.g., GPT-4, Claude-3) |\n",
    "| System Message | Sets the overall behavior (\"You are a helpful assistant.\") |\n",
    "| User Message | The actual task or question you want answered |\n",
    "| Additional Parameters | Fine-tune behavior, like: <br> üîπ `temperature` ‚Äî randomness control (0 = serious, 1 = creative) <br> üîπ `top_p`, `max_tokens`, etc. |\n",
    "\n",
    "‚úÖ **Temperature Tip:**  \n",
    "- **Low (0-0.3)** ‚Üí Focused, factual, serious  \n",
    "- **Medium (0.5)** ‚Üí Balanced, reasonable creativity  \n",
    "- **High (0.8-1)** ‚Üí Wild, creative, surprising\n",
    "\n",
    "---\n",
    "\n",
    "# üöÄ Coming Up\n",
    "\n",
    "We'll **put these LLMs to much smarter uses** ‚Äî  \n",
    "not just laughing at robot jokes, but building, reasoning, and creating!\n",
    "\n",
    "‚ú®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0298750-1207-4361-b86f-e28247529209",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of AI Bigenners of CapitalOne\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a260d1-7291-427a-9ed8-631596230948",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3ecb60-555a-4514-b05e-5e72c65ea8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-3.5-Turbo\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bfb627-9832-49c0-b3e3-37bf75e4eff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4o-mini\n",
    "# Temperature setting controls creativity\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfc8bac-cf7d-4c4d-8aa9-5dc3b81d98d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4o\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcb86ad-8f7d-41d2-9a61-38f81341614b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 3.5 Sonnet\n",
    "# API needs system message provided separately from user prompt\n",
    "# Also adding max_tokens\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-3-5-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83a9b7a-4551-4bbc-8680-f14d20dc7dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 3.5 Sonnet again\n",
    "# Now let's add in streaming back results\n",
    "# If the streaming looks strange, then please see the note below this cell!\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-3-5-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7123364-e686-40ed-8a08-1f749653352d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The API for Gemini has a slightly different structure.\n",
    "# I've heard that on some PCs, this Gemini code causes the Kernel to crash.\n",
    "# If that happens to you, please skip this cell and use the next cell instead - an alternative approach.\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-2.0-flash-exp',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39975bb-2439-4759-9c1a-16c4c3e97825",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# üß† Lab: Back to OpenAI with a Serious Question\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Objective\n",
    "\n",
    "In this lab, we return to OpenAI ‚Äî  \n",
    "but this time, instead of casual prompts,  \n",
    "you will ask the model a **serious and thoughtful question**.\n",
    "\n",
    "This exercise is designed to help you:\n",
    "\n",
    "- See how LLMs handle **deeper, meaningful queries**  \n",
    "- Observe **differences in reasoning and structure** compared to simple Q&A  \n",
    "- Practice **formulating precise prompts** for better outputs\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Instructions\n",
    "\n",
    "1. **Think of a serious question** you'd genuinely want insight into.  \n",
    "   *(Example: \"What are the ethical risks of AI in healthcare?\" or \"How can startups leverage LLMs for customer service?\")*\n",
    "\n",
    "2. **Craft a clear, specific prompt** that reflects your intent.\n",
    "\n",
    "3. **Submit your question** to two models of your choice\n",
    "\n",
    "4. **Analyze the model‚Äôs response**:\n",
    "   - Was the answer well-organized?\n",
    "   - Did it seem thoughtful or surface-level?\n",
    "   - Were there gaps or missing considerations?\n",
    "\n",
    "5. **Optional Challenge**:  \n",
    "   - Try adjusting the **temperature** setting to see how it affects the tone of the answer.\n",
    "   - Lower temperature = more serious, structured answers.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Tips for Asking a Good Serious Question\n",
    "\n",
    "| Tip | Why It Matters |\n",
    "|:---|:---|\n",
    "| Be specific | Vague questions get vague answers. |\n",
    "| Provide context | Helps the model reason better. |\n",
    "| Focus on insight | Aim for thoughtful, not trivial, replies. |\n",
    "\n",
    "---\n",
    "\n",
    "# üåü Goal\n",
    "\n",
    "By the end of this lab,  \n",
    "you'll understand how **prompt quality** directly impacts **response quality**,  \n",
    "and how LLMs can **support real-world problem solving**, not just simple chatting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b94888-54bd-49ac-921e-b273854e0f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"set your system message\"\n",
    "user_prompt = \"ask a serious question\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60122823-212f-4f54-a2f3-2680551ba8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450f54a9-bc87-432c-b4cc-345c98ab422b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ask model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e6f4a6-9894-468f-ba39-c70c2ef3109e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ask model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f60ae5-23fa-41c9-a713-0cd207e9e11b",
   "metadata": {},
   "source": [
    "## Which one do you like?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
