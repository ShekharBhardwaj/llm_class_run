{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0f99056-0935-48bc-b005-30cb1d987dd5",
   "metadata": {},
   "source": [
    "\n",
    "# 🤖 Hugging Face Models: Running Inference Like a Pro\n",
    "\n",
    "---\n",
    "<img src=\"../models.png\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "\n",
    "## 🎯 Where We Are\n",
    "\n",
    "- ✅ Mastered **Pipelines** (easy-mode inference)\n",
    "- ✅ Learned **Tokenizers** (how text becomes tokens)\n",
    "- 🚀 Now: **Working directly with Models** to generate outputs!\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ What We’re Learning Today\n",
    "\n",
    "| Topic | What It Means |\n",
    "|:---|:---|\n",
    "| **Model Class** | Directly create and run a Hugging Face Transformer model |\n",
    "| **Quantization** | Shrink models to fit on smaller GPUs |\n",
    "| **Looking Inside Models** | Peek at the PyTorch layers under the hood |\n",
    "| **Streaming Outputs** | Get generated text piece-by-piece as it forms |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔥 Models We’ll Use Today\n",
    "\n",
    "| Model | Special Sauce |\n",
    "|:---|:---|\n",
    "| **LLaMA 3.1** (Meta) | Open-source powerhouse |\n",
    "| **Phi 3** (Microsoft) | Small, efficient, smart |\n",
    "| **Gemma** (Google) | The \"mini Gemini\" cousin |\n",
    "| **Mistral** (Mistral AI) | Lightweight speedster |\n",
    "| **Qwen 2** (Alibaba) | Bilingual, benchmark leader |\n",
    "\n",
    "✅ We'll try 3 models together.  \n",
    "✅ 2 models will be **bonus missions** for you all (Shekhar please)!\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ What is Quantization?\n",
    "\n",
    "- **Normal Model** = Heavy and memory-hungry (32-bit floats)\n",
    "- **Quantized Model** = Slimmer, faster version (4-bit, 8-bit)\n",
    "\n",
    "✅ Helps fit giant models on normal GPUs.  \n",
    "✅ Super important for training and running models cheaply.\n",
    "\n",
    "| Without Quantization | With Quantization |\n",
    "|:---|:---|\n",
    "| Needs huge server | Fits on your gaming laptop |\n",
    "| Slow and expensive | Fast and wallet-friendly |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Looking Inside a Model\n",
    "\n",
    "- Models aren't just black boxes!\n",
    "- We'll peek into their **PyTorch layers**:\n",
    "  - Linear layers\n",
    "  - Attention heads\n",
    "  - Layer norms\n",
    "\n",
    "✅ Just a quick tour — no need to panic (yet).\n",
    "\n",
    "---\n",
    "\n",
    "## 🔥 Streaming Outputs\n",
    "\n",
    "- Instead of waiting for the entire paragraph,\n",
    "- You **stream words as they’re generated**.\n",
    "  \n",
    "✅ Great for building chatbots and assistants!\n",
    "\n",
    "---\n",
    "\n",
    "# 🎯 Final Thought\n",
    "\n",
    "> **Pipelines were riding a bike with training wheels.  \n",
    "> Tokenizers were the map.  \n",
    "> Models are where you drive the spaceship yourself! 🚀**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
