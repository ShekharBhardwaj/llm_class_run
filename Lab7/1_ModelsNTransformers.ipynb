{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0f99056-0935-48bc-b005-30cb1d987dd5",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ¤– Hugging Face Models: Running Inference Like a Pro\n",
    "\n",
    "---\n",
    "<img src=\"../models.png\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "\n",
    "## ðŸŽ¯ Where We Are\n",
    "\n",
    "- âœ… Mastered **Pipelines** (easy-mode inference)\n",
    "- âœ… Learned **Tokenizers** (how text becomes tokens)\n",
    "- ðŸš€ Now: **Working directly with Models** to generate outputs!\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ› ï¸ What Weâ€™re Learning Today\n",
    "\n",
    "| Topic | What It Means |\n",
    "|:---|:---|\n",
    "| **Model Class** | Directly create and run a Hugging Face Transformer model |\n",
    "| **Quantization** | Shrink models to fit on smaller GPUs |\n",
    "| **Looking Inside Models** | Peek at the PyTorch layers under the hood |\n",
    "| **Streaming Outputs** | Get generated text piece-by-piece as it forms |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¥ Models Weâ€™ll Use Today\n",
    "\n",
    "| Model | Special Sauce |\n",
    "|:---|:---|\n",
    "| **LLaMA 3.1** (Meta) | Open-source powerhouse |\n",
    "| **Phi 3** (Microsoft) | Small, efficient, smart |\n",
    "| **Gemma** (Google) | The \"mini Gemini\" cousin |\n",
    "| **Mistral** (Mistral AI) | Lightweight speedster |\n",
    "| **Qwen 2** (Alibaba) | Bilingual, benchmark leader |\n",
    "\n",
    "âœ… We'll try 3 models together.  \n",
    "âœ… 2 models will be **bonus missions** for you all (Shekhar please)!\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ› ï¸ What is Quantization?\n",
    "\n",
    "- **Normal Model** = Heavy and memory-hungry (32-bit floats)\n",
    "- **Quantized Model** = Slimmer, faster version (4-bit, 8-bit)\n",
    "\n",
    "âœ… Helps fit giant models on normal GPUs.  \n",
    "âœ… Super important for training and running models cheaply.\n",
    "\n",
    "| Without Quantization | With Quantization |\n",
    "|:---|:---|\n",
    "| Needs huge server | Fits on your gaming laptop |\n",
    "| Slow and expensive | Fast and wallet-friendly |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ” Looking Inside a Model\n",
    "\n",
    "- Models aren't just black boxes!\n",
    "- We'll peek into their **PyTorch layers**:\n",
    "  - Linear layers\n",
    "  - Attention heads\n",
    "  - Layer norms\n",
    "\n",
    "âœ… Just a quick tour â€” no need to panic (yet).\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¥ Streaming Outputs\n",
    "\n",
    "- Instead of waiting for the entire paragraph,\n",
    "- You **stream words as theyâ€™re generated**.\n",
    "  \n",
    "âœ… Great for building chatbots and assistants!\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸŽ¯ Final Thought\n",
    "\n",
    "> **Pipelines were riding a bike with training wheels.  \n",
    "> Tokenizers were the map.  \n",
    "> Models are where you drive the spaceship yourself! ðŸš€**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
