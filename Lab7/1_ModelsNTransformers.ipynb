{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0f99056-0935-48bc-b005-30cb1d987dd5",
   "metadata": {},
   "source": [
    "\n",
    "# 🤖 Hugging Face Models: Running Inference Like a Pro\n",
    "\n",
    "---\n",
    "<img src=\"../models.png\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "## 🎯 Where We Are\n",
    "\n",
    "- ✅ Mastered **Pipelines** (easy-mode inference)\n",
    "- ✅ Learned **Tokenizers** (how text becomes tokens)\n",
    "- 🚀 Now: **Working directly with Models** to generate outputs!\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ What We’re Learning Today\n",
    "\n",
    "| Topic | What It Means |\n",
    "|:---|:---|\n",
    "| **Model Class** | Directly create and run a Hugging Face Transformer model |\n",
    "| **Quantization** | Shrink models to fit on smaller GPUs |\n",
    "| **QLoRA** | Magic trick to fine-tune giant models on tiny machines |\n",
    "| **Looking Inside Models** | Peek at the PyTorch layers under the hood |\n",
    "| **Streaming Outputs** | Get generated text piece-by-piece as it forms |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔥 Models We’ll Use Today\n",
    "\n",
    "| Model | Special Sauce |\n",
    "|:---|:---|\n",
    "| **LLaMA 3.1** (Meta) | Open-source powerhouse |\n",
    "| **Phi 3** (Microsoft) | Small, efficient, smart |\n",
    "| **Gemma** (Google) | The \"mini Gemini\" cousin |\n",
    "| **Mistral** (Mistral AI) | Lightweight speedster |\n",
    "| **Qwen 2** (Alibaba) | Bilingual, benchmark leader |\n",
    "\n",
    "✅ We'll explore 3 models together.  \n",
    "✅ 2 models are bonus missions for you to conquer!\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ What is Quantization?\n",
    "\n",
    "- **Normal Model** = Huge memory eater (32-bit precision)\n",
    "- **Quantized Model** = Diet version (4-bit or 8-bit)\n",
    "\n",
    "✅ Helps giant models **fit on normal GPUs**  \n",
    "✅ **Faster** loading and **cheaper** inference\n",
    "\n",
    "| Without Quantization | With Quantization |\n",
    "|:---|:---|\n",
    "| Needs supercomputer | Fits on gaming laptops |\n",
    "| $$$ | $ |\n",
    "\n",
    "---\n",
    "\n",
    "## 🎩 What is QLoRA?\n",
    "\n",
    "- **QLoRA = Quantized + Low-Rank Adaptation**.\n",
    "- Fine-tune giant models on **modest hardware** without burning down your laptop.\n",
    "- QLoRA tweaks just a **small slice** of the model during training.\n",
    "- Makes fine-tuning **affordable**, **fast**, and **accessible** to mortals (i.e., us).\n",
    "\n",
    "> **Think of it like teaching an elephant ballet — but only moving its toes instead of its whole body!**\n",
    "\n",
    "✅ We'll use QLoRA later to fine-tune models ourselves!  \n",
    "✅ Huge deal for open source innovation.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Looking Inside a Model\n",
    "\n",
    "- Models = Layers of PyTorch magic:  \n",
    "  - Linear transformations\n",
    "  - Self-attention heads\n",
    "  - Normalization layers\n",
    "\n",
    "✅ We'll peek but not panic.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔥 Streaming Outputs\n",
    "\n",
    "- Instead of waiting for the full novel,  \n",
    "- Get **one word at a time** as the model thinks.\n",
    "\n",
    "✅ Makes apps **snappy** and **interactive**.\n",
    "\n",
    "---\n",
    "\n",
    "# 🎯 Final Thought\n",
    "\n",
    "> **Pipelines were riding bikes with training wheels.  \n",
    "> Tokenizers were the map.  \n",
    "> Models are where you drive the spaceship yourself! 🚀**\n",
    "\n",
    "> **Meet me here:** https://colab.research.google.com/drive/1KWsyFt1KHQyWQJn72k7VrFUNO-QMLzL8?usp=sharing\n",
    "---\n",
    "\n",
    "# (Smooth Talking Points for parties)\n",
    "\n",
    "---\n",
    "\n",
    "- \"Remember: models only understand tokens, so choosing the right tokenizer matters — it's like matching the right fuel to the engine.\"\n",
    "- \"Quantization is like squeezing a huge fluffy pillow into a backpack — it’s smaller, lighter, but still comfy!\"\n",
    "- \"QLoRA lets you **fine-tune like a boss** without needing a $10,000 GPU.\"\n",
    "- \"Streaming is like live-translating a speech instead of waiting for someone to write the whole book.\"\n",
    "- \"We run models, tinker with models, and soon — **we’ll OWN the models**!\"\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02533245-6fbb-4df3-8aa0-fb5fb9b3e6a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
